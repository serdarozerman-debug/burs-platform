# -*- coding: utf-8 -*-
import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from dotenv import load_dotenv
import anthropic
import json

load_dotenv('.env.local')

# Supabase
supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
supabase_key = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
supabase: Client = create_client(supabase_url, supabase_key)

# Anthropic Claude API
claude = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

# Scrape edilecek siteler
SITES = [
    "https://www.tev.org.tr/",
    "https://www.vkv.org.tr/",
    "https://www.turkiyeburslari.gov.tr/",
]

def fetch_page(url):
    """SayfayÄ± indir"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ Sayfa indirilemedi ({url}): {e}")
        return None

def ai_parse_scholarships(html, url):
    """Claude AI ile burs bilgilerini Ã§Ä±kar"""
    
    # HTML'i kÄ±salt (token limiti iÃ§in)
    soup = BeautifulSoup(html, 'html.parser')
    
    # Sadece text content al
    text = soup.get_text(separator='\n', strip=True)
    
    # Ã‡ok uzunsa kÄ±salt (ilk 10000 karakter)
    if len(text) > 10000:
        text = text[:10000]
    
    prompt = f"""Bu web sayfasÄ±ndan burs bilgilerini Ã§Ä±kar. JSON array dÃ¶ndÃ¼r:

Web sayfasÄ± iÃ§eriÄŸi:
{text}

Kaynak URL: {url}

Ã‡Ä±kar:
- title: Burs adÄ±
- organization: Kurum adÄ± (URL'den Ã§Ä±kar)
- description: AÃ§Ä±klama (kÄ±sa)
- amount: Miktar (sadece sayÄ±, TL/dolar iÅŸaretlerini Ã§Ä±kar)
- amount_type: "aylÄ±k" veya "yÄ±llÄ±k" veya "tek seferlik"
- deadline: Son baÅŸvuru (YYYY-MM-DD formatÄ±nda, bulamazsan null)
- type: "akademik" veya "ihtiyaÃ§" veya "engelli" veya "sporcu"
- education_level: "lise" veya "lisans" veya "yÃ¼kseklisans" veya "doktora"
- application_url: BaÅŸvuru linki (bulamazsan kaynak URL kullan)

Sadece JSON dÃ¶ndÃ¼r, baÅŸka aÃ§Ä±klama yapma.
"""

    try:
        message = claude.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        response_text = message.content[0].text
        
        # JSON parse et
        scholarships = json.loads(response_text)
        
        return scholarships
        
    except Exception as e:
        print(f"âŒ AI parsing hatasÄ±: {e}")
        return []

def save_to_supabase(scholarships):
    """Supabase'e kaydet"""
    saved = 0
    skipped = 0
    
    for s in scholarships:
        try:
            # Zorunlu alanlarÄ± kontrol et
            if not s.get('title') or not s.get('organization'):
                print(f"âš ï¸  Eksik alan, atlandÄ±: {s}")
                continue
            
            # is_active ekle
            s['is_active'] = True
            
            # Duplicate kontrolÃ¼
            existing = supabase.table('scholarships')\
                .select('id')\
                .eq('title', s['title'])\
                .eq('organization', s['organization'])\
                .execute()
            
            if not existing.data:
                supabase.table('scholarships').insert(s).execute()
                print(f"âœ… Eklendi: {s['title']} ({s['organization']})")
                saved += 1
            else:
                print(f"â­ï¸  Zaten var: {s['title']}")
                skipped += 1
                
        except Exception as e:
            print(f"âŒ KayÄ±t hatasÄ±: {e}")
    
    print(f"\nğŸ“Š Ã–zet: {saved} yeni, {skipped} mevcut")
    return saved

def scrape_all_sites():
    """TÃ¼m siteleri scrape et"""
    total_saved = 0
    
    for url in SITES:
        print(f"\nğŸ•·ï¸  Scraping: {url}")
        print("â”€" * 50)
        
        # SayfayÄ± indir
        html = fetch_page(url)
        if not html:
            continue
        
        # AI ile parse et
        scholarships = ai_parse_scholarships(html, url)
        print(f"ğŸ“Š {len(scholarships)} burs bulundu")
        
        # Kaydet
        if scholarships:
            saved = save_to_supabase(scholarships)
            total_saved += saved
    
    return total_saved

if __name__ == "__main__":
    print("ğŸš€ Universal AI Scraper BaÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    total = scrape_all_sites()
    
    print("\n" + "=" * 50)
    print(f"âœ… TOPLAM: {total} yeni burs eklendi!")