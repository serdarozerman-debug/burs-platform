"""
AÅAMA 1: BURS VEREN KURUMLARI KEÅFEDÄ°CÄ°
Bu script TÃ¼rkiye'deki burs veren kurumlarÄ± bulur ve listeler
"""

import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from dotenv import load_dotenv
import re
import json
from datetime import datetime

load_dotenv('.env.local')

# Supabase connection
url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
key = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
supabase: Client = create_client(url, key)

# Bilinen kurum listesi (manuel baÅŸlangÄ±Ã§)
KNOWN_ORGANIZATIONS = [
    {
        'name': 'TÃœBÄ°TAK',
        'website': 'https://www.tubitak.gov.tr/tr/burslar',
        'category': 'kamu',
        'description': 'TÃ¼rkiye Bilimsel ve Teknolojik AraÅŸtÄ±rma Kurumu'
    },
    {
        'name': 'TÃ¼rk EÄŸitim VakfÄ±',
        'website': 'https://www.tev.org.tr',
        'category': 'vakÄ±f',
        'description': 'EÄŸitim alanÄ±nda Ã§alÄ±ÅŸan kÃ¶klÃ¼ vakÄ±f'
    },
    {
        'name': 'Vehbi KoÃ§ VakfÄ±',
        'website': 'https://www.vkv.org.tr',
        'category': 'vakÄ±f',
        'description': 'EÄŸitim, saÄŸlÄ±k ve kÃ¼ltÃ¼r alanlarÄ±nda destek veren vakÄ±f'
    },
    {
        'name': 'SabancÄ± VakfÄ±',
        'website': 'https://www.sabancivakfi.org',
        'category': 'vakÄ±f',
        'description': 'EÄŸitim ve sosyal geliÅŸim programlarÄ± sunan vakÄ±f'
    }
]

def scrape_isinolsun():
    """isinolsun.com'dan burs veren kurumlarÄ± scrape et"""
    print("\nğŸ“¡ isinolsun.com taranÄ±yor...")
    
    url = "https://isinolsun.com/blog/2024te-universite-ogrencilerine-burs-veren-kurumlar/"
    organizations = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # KurumlarÄ± bul - makale iÃ§eriÄŸinde baÅŸlÄ±klar ve linkler olabilir
        content = soup.find(['article', 'div'], class_=re.compile('content|post|article'))
        
        if content:
            # BaÅŸlÄ±klarÄ± ve linkleri tara
            headings = content.find_all(['h2', 'h3', 'strong', 'b'])
            links = content.find_all('a', href=True)
            
            for heading in headings:
                text = heading.get_text(strip=True)
                # VakÄ±f, dernek, kurum isimleri filtrele
                keywords = ['vakf', 'dernek', 'kurum', 'Ã¼niversite', 'belediye', 'burs']
                if any(k in text.lower() for k in keywords) and len(text) > 5:
                    # YakÄ±ndaki link'i bul
                    next_link = heading.find_next('a', href=True)
                    website = next_link['href'] if next_link and next_link['href'].startswith('http') else None
                    
                    organizations.append({
                        'name': text[:100],
                        'website': website,
                        'category': 'vakÄ±f' if 'vakf' in text.lower() else 'diÄŸer',
                        'description': f'{text} - isinolsun.com listesinden',
                        'source': 'isinolsun.com'
                    })
        
        print(f"âœ… {len(organizations)} kurum bulundu")
        return organizations
        
    except Exception as e:
        print(f"âš ï¸  Hata: {e}")
        return []

def scrape_microfon():
    """microfon.co'dan burs veren kurumlarÄ± scrape et"""
    print("\nğŸ“¡ microfon.co taranÄ±yor...")
    
    url = "https://microfon.co/scholarship"
    organizations = set()
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Burs kartlarÄ±ndaki kurum isimlerini bul
        # FarklÄ± selector'larÄ± dene
        selectors = [
            '.scholarship-card',
            'div[class*="card"]',
            'article',
            'div[class*="burs"]'
        ]
        
        for selector in selectors:
            items = soup.select(selector)
            if items:
                print(f"âœ… {len(items)} burs kartÄ± bulundu")
                for item in items[:20]:  # Ä°lk 20 kart
                    # Kurum ismini bul
                    org_elem = item.find(['h3', 'h4', 'strong', 'span'], class_=re.compile('org|kurum|institution'))
                    if not org_elem:
                        # TÃ¼m text'i tara
                        text = item.get_text()
                        # "VakfÄ±", "DerneÄŸi" ile biten satÄ±rlarÄ± bul
                        matches = re.findall(r'([A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼\s]+(?:VakfÄ±|DerneÄŸi|Kurumu|Ãœniversitesi|Belediyesi))', text)
                        for match in matches:
                            organizations.add(match.strip())
                    else:
                        organizations.add(org_elem.get_text(strip=True))
                break
        
        org_list = [{'name': org, 'source': 'microfon.co'} for org in organizations if len(org) > 3]
        print(f"âœ… {len(org_list)} benzersiz kurum bulundu")
        return org_list
        
    except Exception as e:
        print(f"âš ï¸  Hata: {e}")
        return []

def google_search_organizations():
    """Google aramasÄ± simÃ¼lasyonu - bilinen kurumlar"""
    print("\nğŸ“¡ Google aramasÄ± simÃ¼lasyonu...")
    
    # GerÃ§ek Google API kullanmak yerine bilinen kurumlarÄ± ekle
    additional_orgs = [
        {
            'name': 'Ä°stanbul BÃ¼yÃ¼kÅŸehir Belediyesi',
            'website': 'https://www.ibb.istanbul',
            'category': 'belediye',
            'description': 'Ä°BB eÄŸitim burslarÄ±'
        },
        {
            'name': 'TÃ¼rkiye Scholarships',
            'website': 'https://www.turkiyeburslari.gov.tr',
            'category': 'kamu',
            'description': 'TÃ¼rkiye BurslarÄ± - Yurt dÄ±ÅŸÄ± Ã¶ÄŸrenciler iÃ§in'
        },
        {
            'name': 'TÃœSEB',
            'website': 'https://www.tuseb.gov.tr',
            'category': 'kamu',
            'description': 'TÃ¼rkiye Su EnstitÃ¼sÃ¼ eÄŸitim burslarÄ±'
        },
        {
            'name': 'DarÃ¼ÅŸÅŸafaka Cemiyeti',
            'website': 'https://www.darussafaka.org',
            'category': 'vakÄ±f',
            'description': 'Tam burs ve barÄ±nma imkanÄ±'
        },
        {
            'name': 'TOBB',
            'website': 'https://www.tobb.org.tr',
            'category': 'Ã¶zel',
            'description': 'TOBB Ãœniversitesi burslarÄ±'
        }
    ]
    
    print(f"âœ… {len(additional_orgs)} ek kurum eklendi")
    return additional_orgs

def merge_and_deduplicate(org_lists):
    """TÃ¼m listeleri birleÅŸtir ve duplikalarÄ± temizle"""
    print("\nğŸ”„ Kurumlar birleÅŸtiriliyor...")
    
    all_orgs = {}
    
    for org_list in org_lists:
        for org in org_list:
            name = org.get('name', '').strip()
            if not name or len(name) < 3:
                continue
            
            # Normalize et (kÃ¼Ã§Ã¼k harf, ekstra boÅŸluklar)
            normalized = re.sub(r'\s+', ' ', name.lower())
            
            if normalized not in all_orgs:
                all_orgs[normalized] = {
                    'name': name,
                    'website': org.get('website'),
                    'category': org.get('category', 'diÄŸer'),
                    'description': org.get('description', ''),
                    'sources': [org.get('source', 'manual')]
                }
            else:
                # Mevcut kaydÄ± gÃ¼ncelle
                if org.get('website') and not all_orgs[normalized]['website']:
                    all_orgs[normalized]['website'] = org.get('website')
                if org.get('source'):
                    all_orgs[normalized]['sources'].append(org.get('source'))
    
    return list(all_orgs.values())

def save_organizations_to_db(organizations):
    """KurumlarÄ± organizations tablosuna kaydet"""
    print("\nğŸ’¾ Kurumlar database'e kaydediliyor...")
    
    saved_count = 0
    skipped_count = 0
    
    for org in organizations:
        try:
            # Duplicate kontrolÃ¼
            existing = supabase.table('organizations')\
                .select('id')\
                .eq('name', org['name'])\
                .execute()
            
            if not existing.data:
                data_to_insert = {
                    'name': org['name'],
                    'website': org['website'],
                    'category': org['category'],
                    'description': org['description'],
                    'is_active': True,
                    'last_scraped': None,
                    'created_at': datetime.now().isoformat()
                }
                
                supabase.table('organizations').insert(data_to_insert).execute()
                print(f"âœ… Eklendi: {org['name']}")
                saved_count += 1
            else:
                print(f"â­ï¸  Zaten var: {org['name']}")
                skipped_count += 1
                
        except Exception as e:
            print(f"âŒ KayÄ±t hatasÄ± ({org['name']}): {e}")
    
    print(f"\nğŸ“Š Ã–zet: {saved_count} eklendi, {skipped_count} atlandÄ±")
    return saved_count

def save_to_json(organizations, filename='organizations.json'):
    """KurumlarÄ± JSON dosyasÄ±na kaydet"""
    with open(f'scripts/{filename}', 'w', encoding='utf-8') as f:
        json.dump(organizations, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ {len(organizations)} kurum {filename} dosyasÄ±na kaydedildi")

if __name__ == "__main__":
    print("ğŸš€ KURUM KEÅÄ°F SCRAPER BAÅLATILIYOR\n")
    print("="*60)
    
    # 1. Bilinen kurumlarla baÅŸla
    print("\nğŸ“‹ Bilinen kurumlar yÃ¼kleniyor...")
    all_organizations = [KNOWN_ORGANIZATIONS]
    
    # 2. isinolsun.com'u tara
    isinolsun_orgs = scrape_isinolsun()
    if isinolsun_orgs:
        all_organizations.append(isinolsun_orgs)
    
    # 3. microfon.co'yu tara
    microfon_orgs = scrape_microfon()
    if microfon_orgs:
        all_organizations.append(microfon_orgs)
    
    # 4. Google aramasÄ± (bilinen ek kurumlar)
    google_orgs = google_search_organizations()
    if google_orgs:
        all_organizations.append(google_orgs)
    
    # 5. BirleÅŸtir ve temizle
    final_organizations = merge_and_deduplicate(all_organizations)
    
    print("\n"+"="*60)
    print(f"ğŸ“Š TOPLAM {len(final_organizations)} BENZERSIZ KURUM BULUNDU")
    print("="*60)
    
    # 6. JSON'a kaydet
    save_to_json(final_organizations)
    
    # 7. Supabase'e kaydet
    print("\nâ„¹ï¸  Supabase'e kaydetmek iÃ§in 'organizations' tablosu oluÅŸturulmalÄ±.")
    print("SQL:")
    print("""
CREATE TABLE IF NOT EXISTS organizations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name TEXT NOT NULL UNIQUE,
    website TEXT,
    category TEXT,
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    last_scraped TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
    """)
    
    # KullanÄ±cÄ± onayÄ± (yorumdan Ã§Ä±kar)
    # if input("\nğŸ’¾ Supabase'e kaydetmek istiyor musunuz? (y/n): ").lower() == 'y':
    #     save_organizations_to_db(final_organizations)
    
    print("\nâœ… KURUM KEÅFÄ° TAMAMLANDI!")
    print(f"ğŸ“ SonuÃ§lar: scripts/organizations.json")

